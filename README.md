# Утилита wget (упрощённый аналог `wget -m`)

## Описание

Утилита для загрузки веб-страниц вместе со всем вложенным контентом (ресурсы, ссылки), реализующая функционал, аналогичный зеркалированию сайта (`wget -m`). Позволяет скачивать HTML-страницы, а также связанные с ними ресурсы (CSS, JS, изображения и др.) и рекурсивно переходить по ссылкам внутри того же домена, сохраняя копию сайта.

---

## Возможности

- Загрузка HTML-страниц и вложенных ресурсов (CSS, JS, изображения и т.д.)
- Рекурсивное скачивание страниц по ссылкам внутри того же домена
- Управление глубиной рекурсии (уровнем вложенности ссылок)
- Устранение дублирования — один и тот же ресурс скачивается один раз
- Корректное формирование локальных путей для сохранения файлов
- Избежание зацикливания по ссылкам
- Обработка ошибок сетевых запросов и операций с файлами
- Лимит таймаута на HTTP-запросы
- Параллельное скачивание с ограничением количества одновременных загрузок


---


1. **Входные параметры**
   - URL начальной страницы
   - Глубина рекурсии (флаг `d`)
   - Число параллельных загрузок (флаг `n`)

2. **Основной функционал**
   - Скачивать страницу по указанному URL
   - Парсить HTML и извлекать:
     - ссылки на другие страницы того же домена
     - ресурсы (CSS, JS, изображения и др.)
   - Для каждой ссылки или ресурса скачивать и сохранять локально
   - Для HTML страниц переписывать ссылки на локальные пути
   - Выполнять рекурсию по ссылкам согласно глубине


3. **Надежность и производительность**
   - Обрабатывает и логгировать ошибки запроса и записи файлов
   - Использует таймауты для HTTP-запросов
   - Реализована параллельность с ограничением по числу одновременных соединений

4. **Флаги**

- `<URL>` — начальный адрес для скачивания
- `-d` — глубина рекурсии (по умолчанию 1)
- `-n` — максимальное число одновременных загрузок (по умолчанию 3)

5. **Использование**

Склонируйте репозиторий и в корне проекта ввести команду скачивания:
```
go run cmd/main.go -d 2 -n 3 google.com
```
```
go run cmd/main.go -d 2 -n 3 go.dev`
```


